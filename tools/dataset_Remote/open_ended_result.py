import os
import json
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
import torch
from transformers import BertTokenizer, BertModel

# è®¾ç½® Hugging Face é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class BertEvaluator:
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½ BERT æ¨¡å‹ (Device: {device})...")
        self.device = device
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased').to(self.device)
        self.model.eval()
        
        # ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—ç›¸åŒå•è¯çš„ Embedding
        self.embedding_cache = {}

    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„ BERT Embeddingï¼Œå¸¦ç¼“å­˜"""
        # é¢„å¤„ç†ï¼šè½¬å°å†™ï¼Œå»æ ‡ç‚¹
        text = str(text).lower().strip().replace('.', '').replace(',', '')
        
        if not text:
            return None
            
        if text in self.embedding_cache:
            return self.embedding_cache[text]

        try:
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=64).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
            # ä½¿ç”¨ [CLS] token
            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
            self.embedding_cache[text] = embedding
            return embedding
        except Exception as e:
            print(f"BERT Error for '{text}': {e}")
            return None

    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if text1 == text2: return 1.0 # å®Œå…¨åŒ¹é…
        
        emb1 = self.get_embedding(text1)
        emb2 = self.get_embedding(text2)
        
        if emb1 is None or emb2 is None:
            return 0.0
            
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)

def calculate_metrics(result_dir, similarity_threshold=0.85):
    """
    è®¡ç®— Precision, Recall, F1
    :param similarity_threshold: BERT ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¤§äºæ­¤å€¼è§†ä¸ºåŒ¹é…æˆåŠŸ
    """
    evaluator = BertEvaluator()
    
    json_files = glob.glob(os.path.join(result_dir, "**/*.json"), recursive=True)
    print(f"ğŸ“‚ æ‰¾åˆ° {len(json_files)} ä¸ªç»“æœæ–‡ä»¶ã€‚")

    # ç»Ÿè®¡æ•°æ®
    # TP: é¢„æµ‹å¯¹äº† (ç›¸ä¼¼åº¦ > é˜ˆå€¼)
    # FP: é¢„æµ‹äº†ä½† GT é‡Œæ²¡æœ‰ (æˆ–è€…ç›¸ä¼¼åº¦éƒ½ä¸å¤Ÿ)
    # FN: GT é‡Œæœ‰ä½†æ²¡é¢„æµ‹å‡ºæ¥
    stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})
    
    global_tp, global_fp, global_fn = 0, 0, 0
    valid_count = 0

    for json_file in tqdm(json_files, desc="Calculating Metrics"):
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
        except:
            continue

        # 1. è·å– Ground Truth
        gt_raw = data.get('gt_object', [])
        # æ¸…æ´— GT åˆ—è¡¨
        gt_objects = set()
        for obj in gt_raw:
            clean_obj = str(obj).lower().strip().replace("['", "").replace("']", "").replace("'", "")
            if clean_obj and clean_obj != "unknown" and clean_obj != "none":
                gt_objects.add(clean_obj)
        
        if not gt_objects:
            continue # æ²¡æœ‰ GT çš„å›¾ç‰‡è·³è¿‡

        # 2. è·å–é¢„æµ‹ç»“æœ
        detections = data.get('detections', [])
        pred_objects = set()
        if isinstance(detections, list):
            for det in detections:
                if isinstance(det, dict) and 'label' in det:
                    clean_label = str(det['label']).lower().strip()
                    pred_objects.add(clean_label)
        
        valid_count += 1

        # 3. åŒ¹é…é€»è¾‘ (åŸºäº BERT ç›¸ä¼¼åº¦çš„äºŒåˆ†å›¾åŒ¹é…ç®€åŒ–ç‰ˆ)
        # æˆ‘ä»¬éœ€è¦çœ‹ GT ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œæ˜¯å¦åœ¨ Pred ä¸­æ‰¾åˆ°äº†â€œè¯­ä¹‰ç›¸ä¼¼â€çš„è¯
        
        # --- è®¡ç®— Recall (é’ˆå¯¹æ¯ä¸ª GT æ‰¾åŒ¹é…) ---
        for gt in gt_objects:
            # åœ¨é¢„æµ‹åˆ—è¡¨ä¸­æ‰¾æœ€ç›¸ä¼¼çš„ä¸€ä¸ª
            best_sim = 0.0
            best_match = None
            
            # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            if gt in pred_objects:
                best_sim = 1.0
                best_match = gt
            else:
                # å¦åˆ™è·‘ BERT
                for pred in pred_objects:
                    sim = evaluator.calculate_similarity(gt, pred)
                    if sim > best_sim:
                        best_sim = sim
                        best_match = pred
            
            # åˆ¤å®š
            if best_sim >= similarity_threshold:
                stats[gt]['tp'] += 1 # å¯¹äºè¿™ä¸ªç±»åˆ«ï¼Œç®— TP
                global_tp += 1
            else:
                stats[gt]['fn'] += 1 # æ²¡æ‰¾åˆ°ç›¸ä¼¼çš„ï¼Œç®— FN
                global_fn += 1

        # --- è®¡ç®— Precision (é’ˆå¯¹æ¯ä¸ª Pred æ‰¾åŒ¹é…) ---
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ã€‚å¦‚æœä¸€ä¸ª Pred åŒ¹é…åˆ°äº†ä»»æ„ä¸€ä¸ª GTï¼Œå°±ç®— TP (ä¸Šé¢å·²ç»åŠ è¿‡äº†)ï¼Œå¦åˆ™ç®— FPã€‚
        # ä¸ºäº†é¿å…é‡å¤è®¡ç®— TPï¼Œæˆ‘ä»¬åªè®¡ç®— FPã€‚
        
        for pred in pred_objects:
            # åœ¨ GT åˆ—è¡¨ä¸­æ‰¾æœ€ç›¸ä¼¼çš„ä¸€ä¸ª
            best_sim = 0.0
            
            if pred in gt_objects:
                best_sim = 1.0
            else:
                for gt in gt_objects:
                    sim = evaluator.calculate_similarity(pred, gt)
                    if sim > best_sim:
                        best_sim = sim
            
            # å¦‚æœæœ€å¤§çš„ç›¸ä¼¼åº¦éƒ½å°äºé˜ˆå€¼ï¼Œè¯´æ˜é¢„æµ‹äº†ä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„ä¸œè¥¿ -> FP
            if best_sim < similarity_threshold:
                # å½’ç±»åˆ° pred è‡ªå·±çš„åå­—ä¸‹
                stats[pred]['fp'] += 1
                global_fp += 1
            # å¦‚æœ >= é˜ˆå€¼ï¼Œä¸Šé¢ Recall é˜¶æ®µå·²ç»ç®—è¿‡ TP äº†ï¼Œè¿™é‡Œä¸é‡å¤åŠ  TP

    # --- è¾“å‡ºç»“æœ ---
    results_list = []
    for cls, metrics in stats.items():
        tp = metrics['tp']
        fp = metrics['fp']
        fn = metrics['fn']
        
        p = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        r = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0.0
        
        results_list.append({
            "Class": cls,
            "Precision": round(p, 4),
            "Recall": round(r, 4),
            "F1": round(f1, 4),
            "Support (TP+FN)": tp + fn
        })

    df = pd.DataFrame(results_list)
    if not df.empty:
        df = df.sort_values(by="Support (TP+FN)", ascending=False)

    # å…¨å±€æŒ‡æ ‡
    micro_p = global_tp / (global_tp + global_fp) if (global_tp + global_fp) > 0 else 0.0
    micro_r = global_tp / (global_tp + global_fn) if (global_tp + global_fn) > 0 else 0.0
    micro_f1 = 2 * (micro_p * micro_r) / (micro_p + micro_r) if (micro_p + micro_r) > 0 else 0.0

    print("\n" + "="*60)
    print(f"ğŸ“Š åŸºäº BERT è¯­ä¹‰ç›¸ä¼¼åº¦çš„è¯„ä¼°ç»“æœ (Threshold={similarity_threshold})")
    print("="*60)
    print(f"Global Precision : {micro_p:.4f}")
    print(f"Global Recall    : {micro_r:.4f}")
    print(f"Global F1-Score  : {micro_f1:.4f}")
    print("="*60)
    
    if not df.empty:
        print(df.head(20).to_string(index=False))
        output_csv = os.path.join(result_dir, "bert_metrics.csv")
        df.to_csv(output_csv, index=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_csv}")

if __name__ == "__main__":
    import argparse
    
    # é»˜è®¤è·¯å¾„ï¼ˆä½ åˆšæ‰æŒ‡å®šçš„è·¯å¾„ï¼‰
    DEFAULT_DIR = "/home/zirui/.cursor-server/Qwen2.5-VL-FT-Remote/results/eval_qwen_instruction/labels/home/zirui/.cursor-server/Qwen2.5-VL-FT-Remote/export_v5_11968/open_ended"

    parser = argparse.ArgumentParser(description='è®¡ç®— BERT è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡')
    parser.add_argument('--dir', type=str, default=DEFAULT_DIR, help='åŒ…å« JSON ç»“æœæ–‡ä»¶çš„ç›®å½•è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.85, help='BERT ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)')
    
    args = parser.parse_args()
    
    if os.path.exists(args.dir):
        print(f"ğŸ¯ æ­£åœ¨è¯„ä¼°ç›®å½•: {args.dir}")
        calculate_metrics(args.dir, similarity_threshold=args.threshold)
    else:
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {args.dir}")
        print("æç¤ºï¼šè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–é€šè¿‡ --dir å‚æ•°æŒ‡å®šã€‚")